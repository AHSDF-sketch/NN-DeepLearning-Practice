# 📚 第01章：导论 - 核心知识点  


## 1.1 课程基础信息  
| 信息类别       | 具体内容                                                                 |
|----------------|--------------------------------------------------------------------------|
| 课程名称       | 神经网络与深度学习                                                       |
| 课程学时       | 32学时                                                                   |
| 课程性质       | 通识扩展（非专业核心课，侧重基础理论与实际应用）                           |
| 核心目标       | 学习用计算机实现人类专属智能，包括问题求解、模式识别、自然语言处理、机器学习等能力 |


## 1.2 神经网络与深度学习概述  
### 1.2.1 神经网络核心定义  
- **灵感来源**：模仿人类大脑神经元的连接与工作方式  
- **核心结构**：由“节点（神经元）+ 层状排列”构成，包含三类层：  
  - 输入层：接收外部数据（如图像像素、文本词向量）  
  - 隐藏层：网络核心，数量/结构决定模型复杂度与表达能力  
  - 输出层：输出模型结果（如分类概率、回归预测值）  
- **关键特性**：  
  - 节点间通过**权重**连接，权重代表连接强度，可通过数据学习自动调整  
  - 基础模型：M-P神经元模型（1943年提出，首个人工神经元模型）  

### 1.2.2 深度学习核心定义  
- **本质**：神经网络的进阶形式，核心特征是“多隐藏层”（深度架构）  
- **核心优势**：**自动化特征提取**——无需人工设计特征，能自动学习数据中复杂、抽象的特征表示  
- **适用场景**：擅长处理高维、复杂数据（如图像、文本、语音）  


## 1.3 神经网络：基本概念与架构  
### 1.3.1 神经元（网络基本单元）  
- **组成要素**（缺一不可）：  
  1. 输入：外部数据源信号或前一层神经元输出  
  2. 权重：每个输入对应一个权重，表征输入的重要程度  
  3. 偏置：额外常数项，调整神经元激活阈值  
  4. 求和函数：计算“输入×权重”总和并叠加偏置（公式：`z = Σ(输入×权重) + 偏置`）  
  5. 激活函数：引入非线性，让模型拟合复杂数据规律  

### 1.3.2 神经网络定义与架构  
- **定义**：模仿生物大脑神经元连接结构的机器学习模型，是深度学习的基础  
- **架构组成**：  
  | 层级       | 功能描述                                                                 |
  |------------|--------------------------------------------------------------------------|
  | 输入层     | 接收原始数据，不做特征处理                                               |
  | 隐藏层     | 核心计算层，逐步提取抽象特征（层数越多，特征越复杂）                       |
  | 全连接层   | 常见层类型，神经元与上一层全连接，捕捉全局特征                             |
  | 输出层     | 输出任务结果（分类→概率，回归→连续值）                                   |
- **典型架构示例**：全连接网络、卷积神经网络（CNN）、深度神经网络（≥8层）  


## 1.4 神经网络与深度学习的发展历程  
按“技术突破节点”分为三大时代，关键时间线如下：  

| 发展时代       | 时间       | 关键事件/模型                          | 核心意义                                  |
|----------------|------------|----------------------------------------|-------------------------------------------|
| 感知器时代     | 1943年     | 提出**M-P神经元模型**                  | 奠定人工神经元理论基础                    |
|                | 1949年     | 提出**赫布学习规则**                    | 确立“权重随关联度调整”的学习逻辑          |
| BP算法时代     | 1986年     | 发表《通过反向传播误差来学习》          | 解决多层网络权重更新难题，推动实用化        |
|                | 1997年     | 提出**LSTM（长短期记忆网络）**          | 缓解RNN梯度消失，适配时序数据（如文本）    |
| 深度学习时代   | 2006年     | 提出**深度置信网络（DBN）**            | 突破深度网络训练瓶颈，开启深度学习时代    |
|                | 2011年     | 提出**ReLU激活函数**                   | 解决Sigmoid梯度消失，成为主流激活函数      |
|                | 2012年     | **AlexNet**获ImageNet冠军               | 验证深度学习在图像任务的优越性，普及CNN    |
|                | 2014年     | 提出**GAN（生成对抗网络）+ GoogLeNet**  | 开启生成式AI，优化网络效率                |
|                | 2015年     | 提出**ResNet（残差网络）**              | 解决网络退化问题，支持超深网络（百层以上）  |
|                | 2017年     | 提出**Transformer模型**                 | 基于自注意力，解决RNN并行性，奠定大模型基础|
|                | 2020年     | 发布**GPT-3**                          | 当时最大预训练语言模型，推动大模型发展    |
|                | 2021年     | 发布**AlphaFold 2**                    | 突破蛋白质结构预测，推动生物医学研究      |
|                | 2022年     | 发布**Stable Diffusion**                | 成为AI绘画标杆，拓展生成式AI场景          |


## 1.5 神经网络与深度学习的关键技术  
### 1.5.1 基础组件技术（模型“零件”）  
| 技术类型     | 核心内容                                                                 |
|--------------|--------------------------------------------------------------------------|
| 激活函数     | - ReLU：计算快、缓解梯度消失，缺点是部分神经元“永久死亡”<br>- Sigmoid：输出0~1，适合二分类概率，深层易梯度消失<br>- Tanh：输出-1~1，零均值更优，仍有梯度消失<br>- Softmax：多分类输出层，转化为概率分布（和为1） |
| 损失函数     | - MSE（均方误差）：回归任务（预测房价/温度），公式`Loss=1/n×Σ(y_true-y_pred)²`<br>- 交叉熵：分类任务，衡量概率分布差异，训练效率高<br>- Triplet Loss：相似性匹配（人脸识别），拉近同类、拉远异类样本 |
| 优化技术     | - 梯度下降：核心算法，沿损失梯度反方向更新参数<br>- 优化器：Adam（自适应学习率）等，提升训练效率<br>- 正则化：Dropout、L2等，缓解过拟合 |

### 1.5.2 架构创新技术（模型“骨架”）  
- **卷积神经网络（CNN）**：图像专用，核心层：  
  - 卷积层：用卷积核提取局部特征（边缘、纹理）  
  - 池化层：最大/平均池化，降维+抗干扰（平移不变性）  
  - 全连接层：整合全局特征，输出结果  
- **Transformer模型**：序列数据（文本）专用，核心组件：  
  - 自注意力：序列内元素互相关注，捕捉全局依赖  
  - 多头注意力：多组自注意力并行，学习多维度关联  
  - 位置编码：补充时序信息（Transformer无循环结构）  


## 1.6 神经网络与深度学习的应用领域  
### 1.6.1 计算机视觉领域  
- 图像识别与分类：识别物体类别（如“猫/狗”“汽车/行人”“手写数字”，识别概率≥94%）  
- 目标检测与追踪：定位物体位置+识别类别，追踪动态物体（如监控追踪行人）  

### 1.6.2 自然语言处理领域  
- 文本理解与分析：提取关键信息（新闻关键词）、判断文本属性（情感倾向）  
- 文本生成：根据需求生成语法通顺、逻辑清晰的文本（文案、摘要）  


## 1.7 总结与展望  
### 1.7.1 核心总结  
- 神经网络：以神经元为单元，通过“前向传播（计算预测）+ 反向传播（更新参数）”实现学习，是深度学习的基础  
- 深度学习：多隐藏层架构+自动化特征提取，依托大数据与算力，在多领域性能卓越  
- 发展逻辑：从理论（M-P模型）→ 实用（BP算法）→ 爆发（深度学习时代），算力与数据是关键推手  

### 1.7.2 未来展望  
- **技术突破**：聚焦模型可解释性（解决“黑箱”问题，明确决策逻辑）  
- **应用拓展**：结合物联网、5G，渗透智能家居（语音控制）、智能交通（自动驾驶）、工业自动化（故障检测）  


## 🚀 跳转至下一章  
完成第01章导论学习后，点击下方章节名称，进入核心优化算法的学习：  
**[第02章：梯度下降](chapters/chter02.md)**
