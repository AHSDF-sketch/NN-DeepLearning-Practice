# 习题10

## 一、单选题（4题）
### 1. 在持续学习中，模型在学习新任务时容易出现哪种现象？
- A、 模型性能提升显著
- B、 模型参数不变
- C、 灾难性遗忘
- D、 模型结构自动扩展
<details>
  <summary>查看答案与解析</summary>
  答案：C<br>
  解析：灾难性遗忘是持续学习中常见的核心问题，指模型在学习新任务时，对已掌握的旧任务知识产生严重遗忘，导致旧任务性能大幅下降。
</details>

### 2. 下列哪一项是持续学习与迁移学习的主要区别？
- A、 迁移学习不考虑模型的稳定性
- B、 持续学习不涉及任务迁移
- C、 持续学习强调在学习新任务时不遗忘旧任务
- D、 迁移学习更注重模型的扩展性
<details>
  <summary>查看答案与解析</summary>
  答案：C<br>
  解析：两者虽均涉及知识迁移，但核心目标不同：持续学习需在学习新任务的同时“保留旧任务性能”，避免灾难性遗忘；迁移学习仅关注将旧任务知识迁移到新任务以提升新任务效率，不强制要求保留旧任务能力。
</details>

### 3. 在持续学习系统中，哪一步骤负责将过去任务的知识用于当前任务的优化？
- A、 接收任务数据
- B、 迁移过去任务知识
- C、 存储当前任务知识
- D、 精炼现存知识
<details>
  <summary>查看答案与解析</summary>
  答案：B<br>
  解析：持续学习系统的核心流程中，“迁移过去任务知识”是连接旧任务与新任务的关键步骤——通过复用旧任务的有效知识（如重要参数、特征表示），优化当前任务的学习效率，同时减少对旧知识的干扰。
</details>

### 4. 弹性权重合并（Elastic Weight Consolidation, EWC）主要用于解决什么问题？
- A、 模型参数过多
- B、 模型训练速度慢
- C、 灾难性遗忘
- D、 模型泛化能力差
<details>
  <summary>查看答案与解析</summary>
  答案：C<br>
  解析：EWC是持续学习中经典的权重正则化方法，通过评估旧任务中“对性能关键的参数”，在学习新任务时限制这些参数的更新幅度，从而减少新任务训练对旧任务知识的覆盖，缓解灾难性遗忘。
</details>


## 二、多选题（4题）
### 5. 持续学习系统的关键过程包括哪些？
- A、 接收任务数据
- B、 迁移过去任务知识
- C、 存储当前任务知识
- D、 精炼现存知识
- E、 删除旧任务数据
<details>
  <summary>查看答案与解析</summary>
  答案：ABCD<br>
  解析：持续学习系统的核心流程为四步：1. 接收任务数据（获取新任务的输入信息）；2. 迁移过去任务知识（复用旧知识优化新任务）；3. 存储当前任务知识（保存新任务的关键信息，供后续任务复用）；4. 精炼现存知识（整合新旧知识，提升知识通用性）；删除旧任务数据会加剧遗忘，不属于关键过程，故E错误。
</details>

### 6. 下列哪些是解决灾难性遗忘的常见策略？
- A、 权重正则化（如EWC）
- B、 梯度正则化（如L2正则化约束梯度）
- C、 经验重放（存储旧任务数据并复用）
- D、 模型扩展（如动态增加网络层/参数）
- E、 增加训练数据
<details>
  <summary>查看答案与解析</summary>
  答案：ABCD<br>
  解析：常见解决策略：权重正则化（限制关键参数更新）、梯度正则化（约束梯度方向，减少对旧知识的破坏）、经验重放（混合新旧数据训练，保留旧知识）、模型扩展（为新任务分配专属参数，不干扰旧参数）；“增加训练数据”是提升模型泛化的通用方法，不针对灾难性遗忘，故E错误。
</details>

### 7. 下列关于弹性权重合并（EWC）的说法，哪些是正确的？
- A、 EWC通过限制重要参数的更新来减少遗忘
- B、 EWC适用于所有类型的神经网络
- C、 EWC的核心是判断哪些参数的变化会增加旧任务损失
- D、 EWC可以完全消除灾难性遗忘
- E、 EWC通过计算参数的二阶导数来评估其重要性
<details>
  <summary>查看答案与解析</summary>
  答案：ACE<br>
  解析：EWC的核心逻辑：1. 计算旧任务中各参数的“重要性”（通过二阶导数评估参数变化对旧任务损失的影响，变化越大会导致旧损失上升的参数越重要）；2. 对重要参数施加正则化约束，限制其在新任务中的更新幅度，从而减少遗忘；B错误（EWC对部分复杂网络的适配性有限，如动态结构网络）；D错误（EWC仅能缓解遗忘，无法完全消除）。
</details>

### 8. 下列哪些是传统机器学习在多任务学习中的缺点？
- A、 学习新任务时容易遗忘旧任务
- B、 模型训练效率高
- C、 需要重新训练所有历史任务（“回放式”训练）
- D、 模型结构固定，无法动态扩展
- E、 模型泛化能力强
<details>
  <summary>查看答案与解析</summary>
  答案：ACD<br>
  解析：传统机器学习多任务学习的缺点：1. 灾难性遗忘（新任务训练覆盖旧参数）；2. 需回放所有旧任务数据重新训练（否则旧任务性能下降），成本高；3. 模型结构固定，无法为新任务分配专属参数；B、E是优点，故错误。
</details>


## 三、判断题（4题）
### 9. 持续学习的目标是让模型在学习新任务时，同时保留对旧任务的知识。
<details>
  <summary>查看答案与解析</summary>
  答案：正确<br>
  解析：持续学习模拟人类“终身学习”能力，核心目标就是在连续学习新任务的过程中，既掌握新任务知识，又不遗忘已学会的旧任务知识，避免灾难性遗忘。
</details>

### 10. 迁移学习和持续学习在本质上是相同的。
<details>
  <summary>查看答案与解析</summary>
  答案：错误<br>
  解析：两者本质差异在于“对旧任务的态度”：迁移学习的核心是“用旧知识帮新任务”，不要求保留旧任务性能；持续学习的核心是“学新任务的同时保旧任务”，必须缓解灾难性遗忘，两者目标和设计逻辑不同。
</details>

### 11. 弹性权重合并（EWC）通过增加模型参数数量来减少灾难性遗忘。
<details>
  <summary>查看答案与解析</summary>
  答案：错误<br>
  解析：EWC的核心是“正则化约束”，通过限制旧任务中“重要参数”的更新幅度来保护旧知识，不涉及增加参数数量；“增加参数”是“模型扩展”策略的思路（如为新任务新增网络层），与EWC无关。
</details>

### 12. 经验重放策略通过存储过去任务的数据并在新任务中重放，帮助模型保留旧知识。
<details>
  <summary>查看答案与解析</summary>
  答案：正确<br>
  解析：经验重放模拟人类“复习”行为，通过存储旧任务的关键数据，在新任务训练时混合新旧数据共同训练，让模型持续接触旧任务信息，从而减少对旧知识的遗忘，是缓解灾难性遗忘的经典数据驱动策略。
</details>


## 四、简答题（4题）
### 13. 请简述持续学习与传统机器学习在多任务学习中的主要区别。
<details>
  <summary>查看答案与解析</summary>
  答案：核心区别在于“对旧任务知识的保留态度”：1. 持续学习：以“学习新任务+保留旧任务性能”为目标，通过权重正则化、经验重放等策略缓解灾难性遗忘，无需重新训练所有旧任务；2. 传统机器学习：学习新任务时易覆盖旧参数，导致旧任务性能大幅下降，若需保留旧任务能力，需回放所有旧任务数据重新训练，成本高、效率低。<br>
  解析：两者的本质差异源于对“终身学习场景”的适配性——传统方法未针对连续任务设计，而持续学习专门解决“连续学习中的遗忘问题”。
</details>

### 14. 请解释什么是灾难性遗忘，并列举两种解决策略。
<details>
  <summary>查看答案与解析</summary>
  答案：定义：灾难性遗忘是指模型在连续学习新任务时，因参数更新覆盖旧任务关键信息，导致对旧任务的性能大幅下降甚至完全丧失的现象；解决策略：1. 权重正则化（如EWC）：评估旧任务关键参数，限制其在新任务中的更新幅度；2. 经验重放：存储旧任务数据，在新任务训练时混合新旧数据，让模型持续复习旧知识。<br>
  解析：灾难性遗忘是持续学习的核心挑战，解决策略均围绕“减少新旧任务对参数的竞争”展开，要么约束参数更新，要么通过数据复用强化旧知识。
</details>

### 15. 请描述弹性权重合并（EWC）的基本原理。
<details>
  <summary>查看答案与解析</summary>
  答案：EWC的核心是“通过参数重要性评估与正则化，保护旧任务知识”，具体步骤：1. 学习旧任务时，计算每个参数的“重要性”（通过二阶导数衡量：参数变化对旧任务损失的影响越大，重要性越高）；2. 学习新任务时，在损失函数中加入正则项：对重要参数施加约束，使其更新幅度受限；3. 新任务训练过程中，重要参数（旧任务关键）变化小，非重要参数可自由更新以适配新任务，从而在学习新任务的同时减少对旧知识的遗忘。<br>
  解析：EWC通过“差异化约束参数更新”，平衡了新任务适配与旧知识保留，是正则化类策略的经典代表。
</details>

### 16. 请简述持续学习系统中“迁移过去任务知识”的作用。
<details>
  <summary>查看答案与解析</summary>
  答案：“迁移过去任务知识”是持续学习系统连接新旧任务的关键环节，作用包括：1. 提升新任务学习效率：复用旧任务中与新任务相关的知识（如通用特征、共享参数），减少新任务的训练数据需求和收敛时间；2. 减少知识冲突：通过定向复用旧知识，避免新任务训练时盲目更新参数，间接缓解灾难性遗忘；3. 提升模型通用性：整合新旧知识形成更通用的特征表示，为后续任务的迁移奠定基础。<br>
  解析：该步骤让持续学习摆脱“孤立学习每个任务”的局限，实现知识的累积与复用，接近人类学习的“循序渐进”特性。
</details>


## 五、填空题（4题）
### 17. 在持续学习中，模型在学习新任务时，如果对旧任务的知识产生严重遗忘，这种现象被称为______。这种现象通常发生在______多任务学习模式中。
<details>
  <summary>查看答案与解析</summary>
  答案：灾难性遗忘；传统（或“非持续学习的”）<br>
  解析：灾难性遗忘是持续学习的核心问题，传统多任务学习因未设计抗遗忘机制，训练新任务时易覆盖旧参数，导致该现象频繁发生；持续学习的目标就是解决这一问题。
</details>

### 18. 弹性权重合并（EWC）通过限制______的更新，减少对旧任务知识的干扰。其核心是计算参数的______，以评估其重要性。
<details>
  <summary>查看答案与解析</summary>
  答案：重要参数（或“旧任务关键参数”）；二阶导数<br>
  解析：EWC的核心逻辑：通过二阶导数判断参数对旧任务的重要性（导数越大，参数变化越影响旧任务损失），进而限制这些重要参数的更新，保护旧知识；若不评估重要性，盲目约束所有参数会导致模型无法适配新任务。
</details>

### 19. 持续学习系统的四个关键过程依次是：接收任务数据、______、存储当前任务知识和______。
<details>
  <summary>查看答案与解析</summary>
  答案：迁移过去任务知识；精炼现存知识<br>
  解析：四步流程形成闭环：“接收数据”是输入，“迁移知识”是复用旧知识，“存储知识”是保存新知识，“精炼知识”是整合新旧知识提升通用性，确保模型在连续任务中持续累积有效知识。
</details>

### 20. 经验重放策略通过在新任务训练中加入______的数据，帮助模型保留旧任务知识。这种方法模拟了人类在学习新知识时______的过程。
<details>
  <summary>查看答案与解析</summary>
  答案：过去任务（或“旧任务”）；复习旧知识（或“回顾已有知识”）<br>
  解析：经验重放的核心是“数据复用”——通过混合新旧数据训练，让模型持续接触旧任务信息，类似人类通过复习巩固记忆的行为，从而减少对旧知识的遗忘，是数据驱动型抗遗忘策略的核心思路。
</details>

