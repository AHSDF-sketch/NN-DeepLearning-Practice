# 习题五

## 一、单选题（4题）
### 1. 在神经网络中，损失函数的主要作用是什么？
- A、 衡量模型的预测值与真实值之间的差距
- B、 增加模型的训练时间
- C、 提高模型的参数数量
- D、 减少模型的输入维度
<details>
  <summary>查看答案与解析</summary>
  答案：A<br>
  解析：损失函数的核心作用是量化模型预测与真实值之间的差异，为模型优化提供方向。
</details>

### 2. 下列哪种损失函数对异常值最敏感？
- A、 均方误差 (MSE)
- B、 平均绝对误差 (MAE)
- C、 交叉熵损失
- D、 Huber损失
<details>
  <summary>查看答案与解析</summary>
  答案：A<br>
  解析：MSE通过平方计算误差，会放大异常值的影响，因此对异常值最敏感。
</details>

### 3. 交叉熵损失函数通常与哪种激活函数搭配使用？
- A、 ReLU
- B、 Sigmoid
- C、 Tanh
- D、 Leaky ReLU
<details>
  <summary>查看答案与解析</summary>
  答案：B<br>
  解析：交叉熵损失常与Sigmoid或Softmax激活函数结合，用于分类任务中概率分布的优化。
</details>

### 4. Huber损失函数的主要特点是什么？
- A、 对异常值完全不敏感
- B、 结合了MSE和MAE的优点
- C、 仅适用于分类任务
- D、 梯度始终为常数
<details>
  <summary>查看答案与解析</summary>
  答案：B<br>
  解析：Huber损失在误差较小时类似MSE，误差较大时类似MAE，平衡了对异常值的敏感性和梯度稳定性。
</details>


## 二、多选题（4题）
### 5. 下列哪些损失函数适用于二分类任务？
- A、 均方误差 (MSE)
- B、 二元交叉熵
- C、 Hinge损失
- D、 多分类SVM损失
<details>
  <summary>查看答案与解析</summary>
  答案：ABC<br>
  解析：MSE、二元交叉熵和Hinge损失均可用于二分类任务，而多分类SVM损失适用于多分类场景。
</details>

### 6. 关于交叉熵损失函数的描述，哪些是正确的？
- A、 随预测概率接近真实标签而减小
- B、 对错误预测的惩罚随误差增大而加剧
- C、 仅适用于线性模型
- D、 与极大似然估计原理相关
<details>
  <summary>查看答案与解析</summary>
  答案：ABD<br>
  解析：交叉熵损失与极大似然估计一致，其值随预测准确度提升而下降，且对严重错误惩罚更重。
</details>

### 7. 下列哪些损失函数是凸函数？
- A、 均方误差 (MSE)
- B、 平均绝对误差 (MAE)
- C、 交叉熵损失
- D、 Huber损失
<details>
  <summary>查看答案与解析</summary>
  答案：ACD<br>
  解析：MSE、交叉熵和Huber损失在特定条件下是凸函数，而MAE虽凸但梯度不连续。
</details>

### 8. Hinge损失函数通常用于哪种机器学习模型？
- A、 逻辑回归
- B、 支持向量机 (SVM)
- C、 决策树
- D、 线性回归
<details>
  <summary>查看答案与解析</summary>
  答案：B<br>
  解析：Hinge损失是支持向量机中的核心损失函数，用于最大化分类间隔。
</details>


## 三、判断题（4题）
### 9. 损失函数在模型训练过程中仅用于评估模型性能，不参与参数更新。
<details>
  <summary>查看答案与解析</summary>
  答案：错误<br>
  解析：损失函数不仅评估性能，其梯度还直接指导模型参数的优化更新。
</details>

### 10. 均方误差 (MSE) 的梯度随误差增大而线性增长。
<details>
  <summary>查看答案与解析</summary>
  答案：正确<br>
  解析：MSE的梯度为“2×(真实值-预测值)”，与误差成正比，误差越大梯度越大。
</details>

### 11. 交叉熵损失函数可用于线性回归任务。
<details>
  <summary>查看答案与解析</summary>
  答案：错误<br>
  解析：交叉熵损失专用于分类任务，回归任务常使用MSE、MAE等损失函数。
</details>

### 12. Huber损失函数在误差较大时梯度恒定。
<details>
  <summary>查看答案与解析</summary>
  答案：正确<br>
  解析：当误差超过阈值δ时，Huber损失转为线性，梯度为常数，减轻异常值影响。
</details>


## 四、简答题（4题）
### 13. 请简述平均绝对误差 (MAE) 的优缺点。
<details>
  <summary>查看答案与解析</summary>
  答案：优点：对异常值不敏感，梯度稳定；缺点：在零点不可导，优化效率较低。<br>
  解析：MAE的绝对值计算使其对异常值鲁棒，但梯度不连续性可能影响收敛速度。
</details>

### 14. 交叉熵损失函数如何解决梯度消失问题？
<details>
  <summary>查看答案与解析</summary>
  答案：通过与Sigmoid/Softmax激活函数结合，其梯度公式中的“真实值”与“预测值”相减，避免了梯度饱和。<br>
  解析：例如，二元交叉熵的梯度为“预测值-真实值”，当预测与真实值差异大时梯度仍显著，促进有效学习。
</details>

### 15. 解释Huber损失函数中超参数δ的作用。
<details>
  <summary>查看答案与解析</summary>
  答案：δ控制从平方损失转向线性损失的阈值。误差小于δ时使用MSE，大于δ时使用MAE，平衡灵敏度和鲁棒性。<br>
  解析：δ越小则越接近MAE，对异常值更鲁棒；δ越大则越接近MSE，对小幅误差更敏感。
</details>

### 16. 为什么在分类任务中交叉熵损失比均方误差更常用？
<details>
  <summary>查看答案与解析</summary>
  答案：交叉熵损失与概率输出匹配，梯度更利于优化；MSE在概率场景下易导致梯度饱和和收敛缓慢。<br>
  解析：交叉熵的梯度直接关联概率误差，而MSE的梯度在概率接近0或1时会减弱，影响训练效率。
</details>


## 五、填空题（4题·修正乱码）
### 17. 均方误差 (MSE) 的公式为“1/n乘以n个样本的（真实值-预测值）的平方和”，其中 ${y_{i}}$ 表示 _____，模型对第i个样本的预测值表示 _____。
<details>
  <summary>查看答案与解析</summary>
  答案：第i个样本的真实值；$\hat{y}_{i}$（模型对第i个样本的预测值）<br>
  解析：${y_{i}}$ 是第i个样本的真实标签或真实输出，$\hat{y}_{i}$ 是模型针对该样本的预测结果。
</details>

### 18. 二元交叉熵损失函数的公式为“-1/n乘以n个样本的[真实标签×log(预测概率) + (1-真实标签)×log(1-预测概率)]的和”，其中 ${y_{i}}$ 表示 _____，$\hat{y}_{i}$ 表示 _____。
<details>
  <summary>查看答案与解析</summary>
  答案：第i个样本的二分类真实标签（0或1）；模型预测第i个样本为正类的概率<br>
  解析：二分类中真实标签仅取0（负类）或1（正类），预测概率需在0-1之间，与Sigmoid激活函数输出匹配。
</details>

### 19. Huber损失函数的规则为“若|真实值-预测值|≤δ，则损失为1/2×(真实值-预测值)²；否则损失为δ×|真实值-预测值| - 1/2×δ²”，其中 ${δ}$ 表示 _____。
<details>
  <summary>查看答案与解析</summary>
  答案：控制损失函数类型切换的阈值超参数<br>
  解析：δ是人工设定的超参数，决定Huber损失从“平方损失”（适用于小误差）转为“线性损失”（适用于大误差）的边界。
</details>

### 20. 梯度下降法中，参数更新规则为“新参数=旧参数-η×损失函数对参数的梯度”，其中 ${η}$ 表示 _____，${∇_{θ}L}$ 表示 _____。
<details>
  <summary>查看答案与解析</summary>
  答案：学习率（控制参数更新步长）；损失函数对参数θ的梯度<br>
  解析：η决定每次参数更新的幅度（步长），梯度∇_{θ}L指示损失函数下降最快的方向，两者共同决定参数更新的效率和收敛性。
</details>
