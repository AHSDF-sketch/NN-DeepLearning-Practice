# 第08章：注意力机制 - 第5部分（含导航栏）
## 8.5 课程总结与导航
### 核心内容1：本章核心知识点总结  
| 知识模块       | 关键结论                                                                 | 
|----------------|--------------------------------------------------------------------------|
| 核心定义       | 1. 本质：模拟人类选择性关注，聚焦重点信息，弱化冗余；<br>2. 核心输出：通过注意力分数加权求和生成上下文向量 | 
| 计算流程       | 1. 三大组件：Query（查询）、Key（键）、Value（值）；<br>2. 三阶段：相关性计算（点积/余弦相似度）→权重归一化（Softmax）→加权求和；<br>3. 关键步骤：得分缩放（除以特征维度平方根），避免Softmax饱和 | 
| 类型与应用     | 1. 自注意力：关注序列内部关联（BERT/GPT）；<br>2. 交叉注意力：连接跨序列信息（机器翻译）；<br>3. 多头注意力：多视角捕捉特征（Transformer核心） |
| NLP实战案例    | 1. 机器翻译：动态关注源语言，解决长句信息丢失；<br>2. 文本摘要：权重识别关键信息，复制-生成机制平衡准确与灵活 |

# 第08章： 注意力机制- 导航栏
## 章节跳转汇总（适配GitHub自主学习平台）  
## 📝 自评小测验（检验学习效果）  
完成第08章学习后，点击自测关键概念：  
**[第07章 - 自评小测验](question08.md)**  

## 🚀 导航栏  
- **[第09章强化学习与深度学习](../Chater08/chter01.md)**  
- **[返回首页](../../../index.md)**
