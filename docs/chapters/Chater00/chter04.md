# 第00章：导论 - 第4部分
## 0.3.1 神经元（网络基本单元）  
### 核心内容：拆解神经元的组成与工作逻辑  
神经元是神经网络的“最小计算单元”，由5个缺一不可的要素构成：  

| 组成要素        | 功能描述                                                                 | 关键参数/公式（纯文本可显）                          |
|-----------------|--------------------------------------------------------------------------|-----------------------------------------------------|
| 输入（Input）   | 接收外部信号（如前一层神经元输出、原始数据特征）                           | 示例：图像单个像素值、文本单个词向量                |
| 权重（Weight）  | 表征输入的“重要程度”，权重越大，对输出影响越强                             | w₁、w₂...wₙ（w₁对应第1个输入的权重，wₙ对应第n个）    |
| 偏置（Bias）    | 调整激活阈值，避免仅依赖输入导致的表达局限                                 | 用字母b表示（类似线性方程的截距）                   |
| 求和函数        | 整合“输入×权重”结果，叠加偏置得线性组合值                                 | z = (x₁×w₁ + x₂×w₂ + ... + xₙ×wₙ) + b                |
| 激活函数        | 引入非线性变换，让模型拟合复杂规律（无激活则退化为线性模型）               | 1. ReLU：f(z) = max(0, z)；2. Sigmoid：f(z) = 1/(1 + e^(-z)) |


### 公式详细解释
1. **线性组合值（z）计算**：  
   把每个输入（x₁到xₙ）和对应的权重（w₁到wₙ）相乘，所有乘积加起来后，再加上偏置b，得到线性组合结果z（比如3个输入时：z = x₁×w₁ + x₂×w₂ + x₃×w₃ + b）。  

2. **ReLU激活函数**：  
   对线性组合值z做“筛选”——如果z大于0，就输出z本身；如果z小于等于0，就输出0（计算简单，能避免模型训练时的梯度消失问题）。  

3. **Sigmoid激活函数**：  
   把z的结果映射到0~1之间（比如z=0时输出0.5，z越大输出越接近1，z越小输出越接近0），适合二分类任务中表示“属于某类的概率”；公式中e^(-z)表示“自然常数e的(-z)次方”，^符号可让(-z)显示在e的右上角。  


### ➡️ 跳转至下一部分  
掌握神经元结构后，点击进入神经网络整体架构的学习：  
**[第00章：导论 - 第5部分](chter05.md)**
