# 第09章：强化学习与深度强化学习 - 第3部分
## 9.3 经典强化学习算法 
### 核心内容：动态规划（有模型）与蒙特卡洛（无模型）  
| 算法类型       | 核心思想与适用场景                                                       | 关键步骤                                                                 | 图示文字解释                        |
|----------------|--------------------------------------------------------------------------|--------------------------------------------------------------------------|-------------------------------------------------------|
| 动态规划（DP）方法 | 1. 适用场景：环境模型已知（状态转移概率P、奖励R可获取），如规则明确的简单游戏；<br>2. 核心思想：利用贝尔曼方程，通过“策略迭代”或“价值迭代”求解最优策略 | 策略迭代步骤：<br>1. 策略评估：计算当前策略下的Vπ（评估状态价值）；<br>2. 策略改进：基于Vπ更新为贪心策略（选价值最大的动作）；<br>3. 重复至策略收敛；<br>价值迭代步骤：<br>1. 直接用贝尔曼最优方程更新V*（无需单独评估）；<br>2. 收敛后提取最优策略 | <img width="1757" height="832" alt="image" src="https://github.com/user-attachments/assets/b8679193-8fd9-41c2-8b35-c8a699e026ea" />|
| 蒙特卡洛（MC）方法 | 1. 适用场景：环境模型未知，如全新冒险游戏（无规则文档）；<br>2. 核心思想：通过完整“轨迹”（从起始到终止状态的序列）学习，用实际累积奖励估计价值函数，无需bootstrapping | 关键步骤：<br>1. 生成轨迹：遵循当前策略，收集多个“状态→动作→奖励”序列；<br>2. 价值估计：对每个状态/动作，计算其在所有轨迹中的平均累积奖励；<br>3. 策略更新：更新为贪心策略（选价值最高的动作） | <img width="1745" height="835" alt="image" src="https://github.com/user-attachments/assets/df009dd5-a34b-4641-8929-7ff2e137f037" />|

### 算法对比总结  
| 对比维度       | 动态规划（DP）                                                           | 蒙特卡洛（MC）                                                       |
|----------------|--------------------------------------------------------------------------|--------------------------------------------------------------------------|
| 模型依赖       | 有模型（需知P和R）                                                       | 无模型（无需环境信息）                                                   |
| 价值估计       | 依赖bootstrapping（用未来价值估计当前价值）                               | 无bootstrapping（用实际轨迹累积奖励）                                     |
| 效率与收敛     | 效率高、收敛快                                                           | 效率低、方差大、收敛慢                                                   |
| 适用场景       | 简单、规则明确的环境                                                     | 复杂、规则未知的环境                                                     |

> 作用：区分有模型与无模型经典算法的核心逻辑、步骤与场景，理解“环境信息是否已知”对算法选择的影响。  

### ➡️ 跳转至下一部分  
掌握经典算法后，点击学习深度强化学习的核心逻辑与改进：  
**[第09章：强化学习与深度强化学习 - 第4部分](chter04.md)**
