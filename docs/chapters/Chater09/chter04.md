# 第09章：强化学习与深度强化学习 - 第4部分
## 9.4 深度强化学习 
### 核心内容：经典算法局限、DQN及其改进、核心挑战  
| 核心要点       | 具体内容                                                                 | 图示文字解释                    |
|----------------|--------------------------------------------------------------------------|-------------------------------------------------------|
| 经典算法的局限 | 维度灾难：当状态空间维度高时（如Atari游戏状态数达10^10000），传统表格型方法（如Q表）无法存储和处理，难以找到最优策略 | <img width="1729" height="867" alt="image" src="https://github.com/user-attachments/assets/081c564d-5361-43bd-bbcf-944e851ac053" />|
| DQN（Deep Q-Network） | 1. 核心思想：用深度神经网络（DNN）替代“价值表”，拟合Qπ(S,A)，解决高维状态问题；<br>2. 关键创新：<br> - 经验回放（Experience Replay）：存储交互轨迹，随机采样训练，降低样本相关性；<br> - 目标网络（Target Network）：单独维护一个参数固定的目标网络，计算目标Q值，稳定训练 | <img width="1752" height="777" alt="image" src="https://github.com/user-attachments/assets/55391418-6050-4b41-af23-a00973e6a7e2" />|
| DQN改进算法    | 1. Double DQN：解决Q值过估计问题，采用“当前网络选动作、目标网络评价值”，分离选择与评估；<br>2. Dueling DQN：将Q网络拆分为“状态价值分支（V(S)）”和“优势函数分支（A(S,A)）”，提升价值估计精度；<br>3. Rainbow DQN：融合6种改进（Double DQN、Dueling DQN等），是DQN系列集大成者 | 标注“当前网络选动作→目标网络算Q值”，旁注“解耦选择与评估，抑制过估计”；<br>“Dueling DQN架构图”：分两支标注“状态价值分支”“优势函数分支”，旁注“细分价值构成，提升估计精度” |
| 核心挑战       | 1. 训练不稳定性：神经网络目标（最小化损失）与强化学习目标（最大化奖励）冲突；<br>2. 样本效率低：需大量环境交互数据；<br>3. 探索与利用权衡：过度探索（试新动作）→奖励不足，过度利用（用已知最优动作）→错过更好策略 |<img width="1756" height="620" alt="image" src="https://github.com/user-attachments/assets/0456f611-f6fe-4913-85f0-5fb4ee87c8af" />|

> 作用：明确深度强化学习的核心创新（用DNN解决维度灾难），拆解DQN及其改进的核心逻辑，正视实际应用中的关键挑战。  

### ➡️ 跳转至下一部分  
掌握深度强化学习核心后，点击学习实际案例、总结与作业：  
**[第09章：强化学习与深度强化学习 - 第5部分](chter05.md)**
