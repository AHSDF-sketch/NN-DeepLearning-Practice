# 第09章：强化学习与深度强化学习 - 第1部分
## 9.1 强化学习概述
### 核心内容：定义、学习范式对比与发展历程  
| 核心要点       | 具体内容                                                                 | 图示文字解释                        |
|----------------|--------------------------------------------------------------------------|-------------------------------------------------------|
| 强化学习定义   | 智能体（Agent）通过与环境（Environment）交互，试错学习最优策略（Policy），以最大化累积奖励（Reward）的机器学习范式；<br>区别于：<br>1. 监督学习：依赖有标签数据训练，聚焦“预测”；<br>2. 无监督学习：发现数据潜在结构，无奖励机制，聚焦“生成/聚类” |<img width="1734" height="859" alt="image" src="https://github.com/user-attachments/assets/e4776dca-0483-4a22-9d63-20018d058234" />|
| 核心任务特性   | 属于“决策型任务”：智能体在动态环境中采取动作→状态转换→获得即时奖励→持续学习，最终最大化长期累积奖励；<br>对比“预测型任务”（监督/无监督）：仅输出结果，无环境交互 | “强化学习概述”模块下“任务流程示意图”：标注“状态S→动作A→奖励R→新状态S'”循环，旁注“决策型任务=动态交互+奖励反馈” |
| 发展历程       | 1. 早期阶段（1950s-1980s）：行为学基础（桑代克试错学习）、数学基础（贝尔曼动态规划）；<br>2. 经典阶段（1990s-2010s）：Q-Learning（无模型学习）、SARSA（在线学习）；<br>3. 深度学习融合阶段（2013-至今）：DQN（DeepMind，融合深度学习处理高维状态）、AlphaGo（2016，击败围棋世界冠军） | <img width="1691" height="859" alt="image" src="https://github.com/user-attachments/assets/339cc679-c4af-4e85-a0c3-c69e03f0fa46" />|

> 作用：明确强化学习的核心定位、与其他学习范式的差异，梳理发展脉络，为后续核心概念与算法铺垫。  

### ➡️ 跳转至下一部分  
理解概述后，点击学习强化学习的核心概念与数学模型：  
**[第09章：强化学习与深度强化学习 - 第2部分](chter02.md)**
