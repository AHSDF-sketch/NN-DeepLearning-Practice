# 第09章：强化学习与深度强化学习 - 第2部分
## 9.2 核心概念与数学模型（  
### 核心内容：关键要素、MDP模型与价值函数  
| 核心要点       | 具体内容                                                                 | 图示文字解释                         |
|----------------|--------------------------------------------------------------------------|-------------------------------------------------------|
| 六大关键要素   | 1. 智能体（Agent）：执行动作的主体（如机器人、游戏AI）；<br>2. 环境（Environment）：智能体交互的外部场景（如迷宫、游戏世界）；<br>3. 状态（State, S）：环境当前情况（如机器人位置、游戏角色坐标）；<br>4. 动作（Action, A）：智能体可执行的操作（如上下左右移动、攻击）；<br>5. 奖励（Reward, R）：环境对动作的反馈（正奖励=达标，负奖励=出错）；<br>6. 策略（Policy, π）：状态到动作的映射（如“状态S→执行动作A”） |<img width="1729" height="856" alt="image" src="https://github.com/user-attachments/assets/a1101053-bd1f-44b5-81f0-7e215071de97" />|
| 马尔可夫决策过程（MDP） | 1. 定义：由（S,A,P,R,γ）五元组组成的数学模型；|<img width="1403" height="884" alt="image" src="https://github.com/user-attachments/assets/15ddaa6f-4f92-4ae9-be6f-b84bbecfedb7" /> |
| 价值函数       | 1. 状态价值函数Vπ(S)：策略π下，从状态S出发的累积期望奖励 |<img width="1722" height="860" alt="image" src="https://github.com/user-attachments/assets/0f3c097a-5b93-46ad-8456-e2f8ac070e73" />|

> 作用：拆解强化学习的核心组件，建立MDP数学框架，通过价值函数量化“状态/动作的长期价值”，为后续算法提供理论基础。  

### ➡️ 跳转至下一部分  
掌握核心模型后，点击学习经典强化学习算法：  
**[第09章：强化学习与深度强化学习 - 第3部分](chter03.md)**
