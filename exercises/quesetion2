# 神经网络与深度学习-梯度下降专项题库（20题·GitHub部署版）
> 基于文件《梯度下降.docx》整理，点击“查看答案与解析”可展开内容

## 一、单选题（4题）
### 1. 在梯度下降法中，负梯度方向表示什么？
- A、 函数值下降最快的方向
- B、 函数值上升最快的方向
- C、 函数值不变的方向
- D、 函数值最小的方向
<details>
  <summary>查看答案与解析</summary>
  答案：A<br>
  解析：负梯度方向是函数值下降最快的方向，梯度下降法正是沿此方向更新参数以最小化损失函数。
</details>

### 2. 随机梯度下降法（SGD）与批量梯度下降法的主要区别是什么？
- A、 每次迭代使用的样本数量不同
- B、 参数更新方向不同
- C、 学习率设置方式不同
- D、 收敛速度始终相同
<details>
  <summary>查看答案与解析</summary>
  答案：A<br>
  解析：SGD每次使用单个样本计算梯度，批量梯度下降使用全部样本，这影响了收敛速度和稳定性。
</details>

### 3. 哪种梯度下降变体能够自动调整学习率？
- A、 随机梯度下降（SGD）
- B、 动量法（Momentum）
- C、 Adam优化器
- D、 批量梯度下降
<details>
  <summary>查看答案与解析</summary>
  答案：C<br>
  解析：Adam通过计算梯度的一阶矩和二阶矩估计，为每个参数自适应调整学习率。
</details>

### 4. 梯度下降法在哪种情况下可能完全停止收敛？
- A、 到达局部最小值
- B、 梯度接近零
- C、 学习率过小
- D、 所有以上情况
<details>
  <summary>查看答案与解析</summary>
  答案：D<br>
  解析：局部最小值、梯度接近零（如鞍点）和学习率过小都可能导致收敛停滞。
</details>


## 二、多选题（3题）
### 5. 下列哪些是梯度下降法的常见变体？
- A、 随机梯度下降（SGD）
- B、 小批量梯度下降
- C、 动量法
- D、 牛顿法
<details>
  <summary>查看答案与解析</summary>
  答案：ABC<br>
  解析：SGD、小批量梯度下降和动量法都是梯度下降的改进版本，而牛顿法是二阶优化方法。
</details>

### 6. 关于动量法（Momentum）的描述，哪些是正确的？
- A、 通过累积历史梯度加速收敛
- B、 减少参数更新的震荡
- C、 完全消除局部最小值问题
- D、 需要设置动量超参数
<details>
  <summary>查看答案与解析</summary>
  答案：ABD<br>
  解析：动量法通过引入动量项加速收敛并减少震荡，但无法完全避免局部最小值问题。
</details>

### 7. 小批量梯度下降的优点包括哪些？
- A、 比SGD更稳定的收敛
- B、 比批量梯度下降更快
- C、 充分利用向量化计算
- D、 保证找到全局最优
<details>
  <summary>查看答案与解析</summary>
  答案：ABC<br>
  解析：小批量梯度下降平衡了计算效率和稳定性，但不能保证全局最优。
</details>

### 8. 学习率衰减策略的作用是什么？
- A、 初期使用大学习率快速收敛
- B、 后期减小学习率提高精度
- C、 完全避免局部最小值
- D、 消除梯度爆炸问题
<details>
  <summary>查看答案与解析</summary>
  答案：AB<br>
  解析：学习率衰减在训练初期加速收敛，后期细化参数调整，但不解决局部最小值或梯度爆炸问题。
</details>


## 三、判断题（4题）
### 9. 梯度下降法可以用于优化非凸函数。
<details>
  <summary>查看答案与解析</summary>
  答案：正确<br>
  解析：虽然梯度下降法可能陷入局部最优，但在非凸函数优化中仍被广泛应用，如深度学习。
</details>

### 10. 随机梯度下降（SGD）的梯度估计总是无偏的。
<details>
  <summary>查看答案与解析</summary>
  答案：正确<br>
  解析：SGD使用单个样本的梯度估计，其期望值等于全批量梯度，因此是无偏估计。
</details>

### 11. Adam优化器不需要手动设置学习率。
<details>
  <summary>查看答案与解析</summary>
  答案：错误<br>
  解析：Adam有默认学习率，但仍需根据任务调整，它通过自适应机制调整的是每个参数的学习率。
</details>

### 12. 梯度下降法在高维空间中更容易收敛到全局最优。
<details>
  <summary>查看答案与解析</summary>
  答案：错误<br>
  解析：高维空间中局部最小值和鞍点更多，梯度下降更易陷入次优解。
</details>


## 四、简答题（4题）
### 13. 请解释梯度下降法中“鞍点”问题及其影响。
<details>
  <summary>查看答案与解析</summary>
  答案：鞍点处梯度为零但非极值点，梯度下降可能在此停滞。高维问题中鞍点比局部最小值更常见，严重减缓收敛。<br>
  解析：鞍点在不同方向上的曲率相反，梯度为零但并非最优解，是优化中的主要挑战之一。
</details>

### 14. 为什么小批量梯度下降比纯SGD更常用？
<details>
  <summary>查看答案与解析</summary>
  答案：小批量梯度下降通过少量样本计算梯度，既利用向量化加速计算，又比SGD减少梯度方差，平衡效率与稳定性。<br>
  解析：典型批量大小在32-256之间，在GPU上能充分发挥并行计算优势。
</details>

### 15. 简述动量法（Momentum）中动量系数γ的作用。
<details>
  <summary>查看答案与解析</summary>
  答案：γ控制历史梯度对当前更新的影响程度（通常0.9）。较大的γ增强惯性，帮助穿越平坦区和局部最小值。<br>
  解析：动量项使梯度更新保持方向一致性，类似物理中的动量效应，加速收敛并减少震荡。
</details>

### 16. 为什么深度学习常采用自适应优化器（如Adam）？
<details>
  <summary>查看答案与解析</summary>
  答案：自适应优化器自动调整各参数的学习率，适应稀疏梯度或不同尺度参数，减少超参数调优负担且通常收敛更快。<br>
  解析：Adam结合动量法和RMSProp的优点，对初始学习率相对鲁棒，适合大规模非凸优化。
</details>


## 五、填空题（4题）
### 17. 梯度下降法的参数更新公式为θ = θ - η × ∇J(θ)，其中η表示 _____，∇J(θ) 表示 _____。
<details>
  <summary>查看答案与解析</summary>
  答案：学习率；损失函数对参数的梯度<br>
  解析：η控制更新步长，∇J(θ) 指示当前点的最陡下降方向。
</details>

### 18. 动量法的参数更新包含两个部分：当前梯度和_____。
<details>
  <summary>查看答案与解析</summary>
  答案：历史梯度累积（动量项）<br>
  解析：动量法引入速度变量v，更新规则为v = γv + η∇J(θ)，θ = θ - v。
</details>

### 19. 当损失函数存在多个局部最小值时，梯度下降法的收敛结果取决于_____。
<details>
  <summary>查看答案与解析</summary>
  答案：参数初始值<br>
  解析：不同的初始参数可能导向不同的局部最小值，这是非凸优化的典型特性。
</details>

### 20. 学习率衰减的常见策略包括阶梯衰减、_____ 和余弦衰减。
<details>
  <summary>查看答案与解析</summary>
  答案：指数衰减<br>
  解析：指数衰减按固定比例周期性减小学习率，平衡收敛速度与精度。
</details>
