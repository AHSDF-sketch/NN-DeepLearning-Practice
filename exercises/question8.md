# 习题八

---

## 第一部分 注意力机制（20题）
### 一、单选题（4题）
#### 1. 在注意力机制中，Query、Key、Value 的作用是什么？
- A、 Query 表示输入数据，Key 表示权重，Value 表示输出
- B、 Query 表示模型当前的关注点，Key 表示待比较对象的特征，Value 表示实际内容
- C、 Query 是输入序列，Key 是输出序列，Value 是中间结果
- D、 Query 表示权重矩阵，Key 表示输入数据，Value 表示输出数据
<details>
  <summary>查看答案与解析</summary>
  答案：B<br>
  解析：Query 是模型当前的关注点（如翻译时当前生成的词），Key 是待匹配的对象特征（如源语言所有词的特征），Value 是实际用于加权的内容（如源语言词的语义向量），三者协同实现“聚焦关键信息”。
</details>

#### 2. 在注意力机制中，Softmax 函数的主要作用是什么？
- A、 计算 Query 与 Key 的点积
- B、 对注意力得分进行归一化，使其总和为 1
- C、 对注意力得分进行缩放，防止梯度爆炸
- D、 直接输出注意力权重
<details>
  <summary>查看答案与解析</summary>
  答案：B<br>
  解析：Softmax 会将 Query 与 Key 计算出的相关性得分转化为概率分布（所有权重和为1），确保后续对 Value 加权时，权重分配合理且可解释。
</details>

#### 3. 下列哪一项是注意力机制的计算步骤？
- A、 Query 与 Key 相乘 → 得分归一化 → 加权 Value 求和
- B、 Key 与 Value 相乘 → 得分归一化 → 加权 Query 求和
- C、 Query 与 Value 相乘 → 得分归一化 → 加权 Key 求和
- D、 Key 与 Key 相乘 → 得分归一化 → 加权 Value 求和
<details>
  <summary>查看答案与解析</summary>
  答案：A<br>
  解析：注意力核心流程：1. 匹配（Query与Key算相关性得分）；2. 归一化（Softmax处理得分）；3. 加权（用归一化后的权重对Value求和，得到注意力输出）。
</details>

#### 4. 在注意力机制中，为什么要对注意力得分进行缩放？
- A、 为了增加计算速度
- B、 为了防止梯度消失，特别是在高维特征空间中
- C、 为了防止梯度爆炸
- D、 为了简化计算
<details>
  <summary>查看答案与解析</summary>
  答案：B<br>
  解析：高维空间中，Query与Key的点积得分易过大，导致 Softmax 输出趋近于0或1（饱和状态），此时梯度接近0（梯度消失）；缩放（如除以√Key维度）可降低得分范围，避免 Softmax 饱和，保证梯度有效。
</details>


### 二、多选题（4题）
#### 5. 注意力机制的三种主要类型包括哪些？
- A、 自注意力（Self-Attention）
- B、 交叉注意力（Cross-Attention）
- C、 多头注意力（Multi-Head Attention）
- D、 局部注意力（Local Attention）
- E、 全局注意力（Global Attention）
<details>
  <summary>查看答案与解析</summary>
  答案：ABC<br>
  解析：核心类型：自注意力（序列内部元素互相关注，如BERT）、交叉注意力（两个序列间关注，如翻译解码器关注编码器输出）、多头注意力（多组独立注意力并行，捕捉多维度关联，Transformer核心）；局部/全局是注意力范围划分，非主要类型。
</details>

#### 6. 下列哪些是注意力机制的应用场景？
- A、 机器翻译
- B、 图像识别
- C、 语音识别
- D、 数据库查询
- E、 文本摘要
<details>
  <summary>查看答案与解析</summary>
  答案：ABCE<br>
  解析：机器翻译（Cross-Attention匹配源/目标语言）、图像识别（注意力聚焦目标区域）、语音识别（关注关键音频片段）、文本摘要（聚焦核心句子）均需“聚焦关键信息”；数据库查询是数据检索技术，与注意力机制无关。
</details>

#### 7. 下列哪些是注意力机制的优点？
- A、 能够自动聚焦关键信息
- B、 提升模型的表达能力和泛化能力
- C、 减少模型训练时间
- D、 避免信息丢失，特别是在处理长序列时
- E、 简化模型结构
<details>
  <summary>查看答案与解析</summary>
  答案：ABD<br>
  解析：优点：自动聚焦关键信息（忽略冗余）、提升表达能力（捕捉细粒度关联）、处理长序列时减少信息丢失（如Transformer用自注意力解决RNN长程依赖）；缺点：增加计算量（训练时间更长）、模型结构更复杂（需Query/Key/Value计算），故C、E错误。
</details>

#### 8. 下列哪些是注意力机制的计算方式？
- A、 点积（Dot Product）
- B、 余弦相似度（Cosine Similarity）
- C、 欧氏距离（Euclidean Distance）
- D、 曼哈顿距离（Manhattan Distance）
- E、 Softmax 函数
<details>
  <summary>查看答案与解析</summary>
  答案：AB<br>
  解析：Query与Key的相关性计算常用点积（如Transformer）、余弦相似度（衡量向量夹角，适用于特征尺度不一致场景）；欧氏/曼哈顿距离是“距离度量”，非“相关性计算”；Softmax是归一化函数，非计算方式，故C、D、E错误。
</details>


### 三、判断题（4题）
#### 9. 注意力机制的核心思想是让模型学会关注输入信息中的关键部分。
<details>
  <summary>查看答案与解析</summary>
  答案：正确<br>
  解析：注意力机制模拟人类“选择性关注”，通过计算相关性为关键信息分配高权重，次要信息分配低权重，核心是“聚焦有用信息，忽略冗余”。
</details>

#### 10. 在注意力机制中，Query 和 Key 的维度必须相同。
<details>
  <summary>查看答案与解析</summary>
  答案：正确<br>
  解析：Query与Key需通过点积、余弦相似度等计算相关性，这些运算要求两者维度一致（如均为512维）；若维度不同，需通过线性层映射到同一维度后再计算。
</details>

#### 11. 注意力机制只能用于自然语言处理任务。
<details>
  <summary>查看答案与解析</summary>
  答案：错误<br>
  解析：注意力机制是通用技术，除自然语言处理（翻译、摘要）外，还可用于图像识别（如Attention U-Net聚焦病变区域）、语音识别（关注关键词音频）、推荐系统（用户关注的商品特征）等多领域。
</details>

#### 12. 多头注意力机制通过多个独立的注意力头，从不同角度分析信息，增强模型的表达能力。
<details>
  <summary>查看答案与解析</summary>
  答案：正确<br>
  解析：多头注意力将Query/Key/Value拆分为多组，每组独立计算注意力，最后拼接结果——不同头可捕捉不同关联（如一句中“主谓”“动宾”关系），比单头注意力更全面，是Transformer性能提升的关键。
</details>


### 四、简答题（4题）
#### 13. 请简述注意力机制的基本原理。
<details>
  <summary>查看答案与解析</summary>
  答案：注意力机制通过“Query-Key-Value”三步实现：1. 匹配：用Query（当前关注点）与所有Key（待比较对象）计算相关性得分，衡量两者关联程度；2. 归一化：用Softmax将得分转化为概率分布（权重和为1），明确关键信息的权重；3. 加权：用归一化后的权重对Value（实际内容）加权求和，得到聚焦关键信息的注意力输出。核心是“按相关性分配权重，突出有用信息”。<br>
  解析：该原理确保模型不平均对待所有信息，而是动态聚焦对当前任务更重要的内容。
</details>

#### 14. 请解释自注意力机制的工作原理，并举例说明其应用场景。
<details>
  <summary>查看答案与解析</summary>
  答案：工作原理：自注意力是“序列内部的注意力”，Query、Key、Value均来自同一序列（如一句话中所有词），通过计算序列内每个元素与其他元素的相关性，捕捉元素间的依赖关系（如“他”指代“小明”）；应用场景：BERT模型（用自注意力捕捉句子中词的语义关联，提升理解能力）、GPT模型（生成文本时用自注意力关注前文已生成内容）、图像分割（自注意力捕捉像素间的空间关联）。<br>
  解析：自注意力解决了传统RNN难以捕捉长程依赖的问题，能并行处理序列元素。
</details>

#### 15. 请简述交叉注意力机制的作用及其在机器翻译中的应用。
<details>
  <summary>查看答案与解析</summary>
  答案：作用：交叉注意力用于“两个不同序列间建立关联”，Query来自一个序列（如翻译的目标序列），Key和Value来自另一个序列（如源语言序列），实现“一个序列对另一个序列的聚焦”；机器翻译应用：在Transformer解码器中，每次生成目标语言的一个词（Query），通过交叉注意力关注源语言序列中与之语义相关的词（如生成“cat”时关注源语言“猫”），确保翻译的准确性，避免脱离源语言语义。<br>
  解析：交叉注意力是跨序列任务的核心，如翻译、图文生成（文本关注图像特征）均依赖该机制。
</details>

#### 16. 请说明多头注意力机制的优势。
<details>
  <summary>查看答案与解析</summary>
  答案：多头注意力的优势：1. 多维度捕捉关联：多个独立注意力头并行计算，每个头可关注不同类型的关联（如一个头关注“主谓”，另一个关注“修饰”关系），比单头更全面；2. 增强表达能力：不同头的输出拼接后，模型能整合多视角信息，提升对复杂数据的理解；3. 提升鲁棒性：避免单头注意力可能遗漏的关键关联，降低对单一关联的依赖。<br>
  解析：多头注意力是Transformer架构超越传统模型的关键设计，在NLP、CV多领域均有广泛应用。
</details>


### 五、填空题（4题）
#### 17. 在注意力机制中，Query 与 Key 的相关性得分通常通过 ______ 或 ______ 计算。
<details>
  <summary>查看答案与解析</summary>
  答案：点积；余弦相似度<br>
  解析：点积计算简单高效（如Transformer默认用点积），余弦相似度更关注向量方向（适用于特征尺度差异大的场景），两者是最常用的相关性计算方式。
</details>

#### 18. 注意力机制的三个主要步骤是 ______、______ 和 ______。
<details>
  <summary>查看答案与解析</summary>
  答案：匹配（计算Query与Key的相关性）；归一化（Softmax处理得分）；加权求和（对Value加权）<br>
  解析：三步环环相扣，“匹配”明确关联程度，“归一化”确保权重合理，“加权求和”生成聚焦关键信息的输出。
</details>

#### 19. 在 Transformer 模型中，______ 是其核心机制，通过多个独立的注意力头从不同角度分析信息。
<details>
  <summary>查看答案与解析</summary>
  答案：多头注意力<br>
  解析：Transformer的编码器和解码器均以多头注意力为核心，替代传统RNN的时序依赖建模，实现并行计算和长程依赖捕捉。
</details>

#### 20. 在注意力机制中，Softmax 函数的作用是将注意力得分转换为 ______，以便进行后续的加权求和。
<details>
  <summary>查看答案与解析</summary>
  答案：概率分布（或“和为1的权重分布”）<br>
  解析：Softmax将任意得分转化为0-1之间的概率，且所有权重总和为1，确保加权时各信息的权重分配可解释、无冗余。
</details>

---

## 第二部分 梯度下降（20题）
### 一、单选题（6题）
#### 21. 在梯度下降法中，梯度的方向表示什么？
- A、 函数值变化最快的方向
- B、 函数值变化最慢的方向
- C、 函数值不变的方向
- D、 函数值最大的方向
<details>
  <summary>查看答案与解析</summary>
  答案：A<br>
  解析：数学上，梯度是函数在某点“方向导数的最大值方向”，即函数值“上升最快”的方向；梯度下降沿梯度反方向更新，实现函数值“下降最快”。
</details>

#### 22. 梯度下降法中，学习率的作用是什么？
- A、 控制参数更新的步长
- B、 决定梯度的方向
- C、 计算损失函数的值
- D、 确定初始参数值
<details>
  <summary>查看答案与解析</summary>
  答案：A<br>
  解析：学习率（η）是参数更新的“步长控制器”，公式为“新参数=旧参数-η×梯度”；η过大可能导致参数震荡发散，η过小则收敛过慢。
</details>

#### 23. 梯度下降法不可能陷入哪种问题？
- A、 局部最优解
- B、 全局最优解
- C、 鞍点
- D、 梯度爆炸
<details>
  <summary>查看答案与解析</summary>
  答案：B<br>
  解析：梯度下降的目标是逼近全局最优，但非凸函数中易陷入局部最优（局部最小值）、鞍点（梯度为零但非极值）；梯度爆炸是参数更新时梯度过大导致的问题；全局最优是“目标”，而非“陷入的问题”。
</details>

#### 24. 在梯度下降中，如何更新参数θ？
- A、 θ = θ - 学习率 × 梯度
- B、 θ = θ + 学习率 × 梯度
- C、 θ = 梯度 / 学习率
- D、 θ = 梯度 × 学习率
<details>
  <summary>查看答案与解析</summary>
  答案：A<br>
  解析：梯度方向是函数上升最快的方向，为使损失函数减小，需沿梯度“反方向”更新参数，学习率控制步长，公式是梯度下降的核心更新规则。
</details>

#### 25. 下列哪项不是梯度下降法的缺点？
- A、 可能陷入局部最优解
- B、 收敛速度快
- C、 可能陷入鞍点
- D、 参数更新缓慢
<details>
  <summary>查看答案与解析</summary>
  答案：B<br>
  解析：梯度下降的缺点包括局部最优、鞍点、更新缓慢（尤其是学习率小时）；“收敛速度快”是优点（相对牛顿法等二阶方法，计算更简单，大规模数据下收敛效率高）。
</details>

#### 26. 梯度下降法的优点不包括下列哪项？
- A、 收敛速度快
- B、 适用于大规模数据集
- C、 容易陷入局部最优解
- D、 计算复杂度低
<details>
  <summary>查看答案与解析</summary>
  答案：C<br>
  解析：优点：收敛速度快（一阶方法，计算量小）、适用于大规模数据（每次迭代仅需计算梯度）、计算复杂度低；“容易陷入局部最优”是缺点，故C错误。
</details>


### 二、多选题（3题）
#### 27. 梯度下降法的执行流程包括哪些步骤？
- A、 随机选择初始参数位置
- B、 计算梯度（损失函数对参数的导数）
- C、 沿梯度反方向更新参数
- D、 判断是否满足终止条件（如损失收敛）
- E、 直接跳跃到函数最小值点
<details>
  <summary>查看答案与解析</summary>
  答案：ABCD<br>
  解析：执行流程：1. 初始化参数（随机或预训练值）；2. 计算当前参数的梯度；3. 按更新公式调整参数；4. 检查是否终止（如损失变化小于阈值、迭代次数达标）；梯度下降是“迭代逼近”过程，无法直接跳跃到最小值，故E错误。
</details>

#### 28. 梯度下降法在实际应用中有哪些用途？
- A、 机器翻译模型训练（如Transformer参数优化）
- B、 图像分类模型训练（如CNN参数更新）
- C、 语音识别模型训练（如RNN参数优化）
- D、 天气预报（如时序模型参数调整）
- E、 股票预测（如回归模型参数优化）
<details>
  <summary>查看答案与解析</summary>
  答案：ABCDE<br>
  解析：梯度下降是深度学习和机器学习的核心优化方法，所有“基于可导损失函数的参数模型”均依赖其优化，包括翻译、图像分类、语音识别、时序预测（天气预报）、回归预测（股票）等场景。
</details>

#### 29. 梯度下降法可能遇到的问题有哪些？
- A、 局部最优解（非凸函数中）
- B、 鞍点（梯度为零但非极值）
- C、 梯度爆炸（学习率过大导致）
- D、 收敛过慢（学习率过小导致）
- E、 无法处理可导损失函数
<details>
  <summary>查看答案与解析</summary>
  答案：ABCD<br>
  解析：常见问题：局部最优、鞍点、梯度爆炸、收敛过慢；梯度下降的前提是“损失函数可导”，无法处理不可导函数是其适用范围限制，而非“遇到的问题”，故E错误。
</details>


### 三、判断题（4题）
#### 30. 梯度下降法总是能找到全局最优解。
<details>
  <summary>查看答案与解析</summary>
  答案：错误<br>
  解析：仅在“凸函数”中，梯度下降能保证找到全局最优；深度学习中损失函数多为非凸（如神经网络），易陷入局部最优或鞍点，无法保证全局最优。
</details>

#### 31. 梯度方向是函数值变化最快的方向。
<details>
  <summary>查看答案与解析</summary>
  答案：正确<br>
  解析：梯度的数学定义是“函数在该点方向导数的最大值方向”，即函数值“上升最快”的方向，是梯度下降法“沿反方向更新”的理论依据。
</details>

#### 32. 学习率越大，梯度下降法的收敛速度越快。
<details>
  <summary>查看答案与解析</summary>
  答案：错误<br>
  解析：学习率需“适度”：过小会收敛过慢，但过大可能导致参数震荡（围绕最优值波动）甚至发散（损失越来越大），并非越大收敛越快。
</details>

#### 33. 梯度下降法适用于所有类型的损失函数。
<details>
  <summary>查看答案与解析</summary>
  答案：错误<br>
  解析：梯度下降依赖“损失函数可导”（需计算梯度），对于不可导的损失函数（如0-1损失），无法直接使用，需用替代损失（如交叉熵）或其他优化方法（如遗传算法）。
</details>


### 四、简答题（4题）
#### 34. 请简述梯度下降法的基本思想。
<details>
  <summary>查看答案与解析</summary>
  答案：梯度下降的核心思想是“迭代逼近最优解”：1. 假设损失函数可导，通过计算参数的梯度，确定函数值下降最快的方向（梯度反方向）；2. 沿该方向以一定步长（学习率控制）更新参数，使损失函数值逐步减小；3. 重复“计算梯度→更新参数”的过程，直到损失收敛到最小值（或满足终止条件）。核心是“利用梯度信息，逐步优化参数”。<br>
  解析：该思想避免了暴力搜索参数的高复杂度，是高效优化的基础。
</details>

#### 35. 梯度下降法在实际应用中可能遇到哪些问题？如何缓解？
<details>
  <summary>查看答案与解析</summary>
  答案：常见问题及缓解方法：1. 局部最优/鞍点：用动量法、Adam等优化器（利用历史梯度突破局部最优）、初始化多个参数起点；2. 梯度爆炸：用梯度裁剪（限制梯度最大值）、学习率衰减（后期减小步长）；3. 收敛过慢：用自适应优化器（Adam、RMSProp）、学习率预热（初期小步长，稳定后增大）；4. 学习率难选择：用学习率调度策略（如余弦衰减、阶梯衰减）。<br>
  解析：通过优化器改进和超参数调整，可大幅缓解梯度下降的固有问题。
</details>

#### 36. 请解释梯度下降法中学习率的作用及其设置原则。
<details>
  <summary>查看答案与解析</summary>
  答案：作用：学习率（η）控制参数更新的步长，直接影响收敛速度和稳定性：η过大可能导致参数震荡发散，η过小则收敛过慢；设置原则：1. 初期用较小学习率（如1e-4）避免发散，稳定后可适当增大；2. 采用衰减策略（如训练中按迭代次数逐步减小），后期精细化参数；3. 结合任务调整（如小数据集用较小η，大数据集可适当增大）；4. 参考同类模型的经验值（如Transformer默认η=1e-4），再通过实验微调。<br>
  解析：学习率是梯度下降最关键的超参数，合理设置是模型训练成功的关键。
</details>

#### 37. 请简述梯度下降法的执行流程。
<details>
  <summary>查看答案与解析</summary>
  答案：完整执行流程：1. 初始化：随机设置模型参数（如权重、偏置）的初始值，或使用预训练参数；2. 前向计算：输入训练数据，计算模型预测值和损失函数值（衡量预测与真实值的差距）；3. 反向传播：计算损失函数对每个参数的梯度（导数），明确参数调整方向；4. 参数更新：按公式“新参数=旧参数-学习率×梯度”沿梯度反方向更新参数；5. 终止判断：若损失函数值收敛（变化小于阈值）或迭代次数达到上限，停止训练；否则返回步骤2，继续迭代。<br>
  解析：流程涵盖“前向计算损失→反向计算梯度→迭代更新参数”，是深度学习训练的标准流程。
</details>


### 五、填空题（3题）
#### 38. 梯度下降法的参数更新公式为：θ = θ - _____ × 梯度，其中空白处为 ______。
<details>
  <summary>查看答案与解析</summary>
  答案：学习率（或η）<br>
  解析：学习率控制更新步长，是公式中连接“梯度”与“参数变化量”的关键超参数，确保更新幅度合理。
</details>

#### 39. 梯度是函数在某一点上变化最快的 ______，梯度下降法沿其 ______ 方向更新参数。
<details>
  <summary>查看答案与解析</summary>
  答案：方向；反（或相反）<br>
  解析：梯度是“方向”而非数值，其方向是函数上升最快的方向，为使损失减小，需沿反方向更新。
</details>

#### 40. 梯度下降法的目标是使 ______ 的值尽可能小，从而提升模型在任务上的性能。
<details>
  <summary>查看答案与解析</summary>
  答案：损失函数<br>
  解析：损失函数衡量模型预测的误差，梯度下降通过优化参数减小损失，最终实现模型性能提升（如分类准确率提高、回归误差降低）。
</details>
